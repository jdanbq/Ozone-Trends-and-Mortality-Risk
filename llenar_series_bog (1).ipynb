{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys # Para la barra de progreso\n",
        "\n",
        "def imputar_modificado(archivo_csv_entrada, archivo_csv_salida):\n",
        "    \"\"\"\n",
        "    Imputa valores faltantes (NaN) en un archivo CSV de series de tiempo.\n",
        "\n",
        "    Los NaN en cada columna (estación) se rellenan utilizando el promedio\n",
        "    de los valores existentes para esa misma hora del día en esa misma estación.\n",
        "    Esta función está adaptada del código original proporcionado.\n",
        "\n",
        "    Args:\n",
        "        archivo_csv_entrada (str): Nombre del archivo CSV de entrada (ej: 'O3.CSV').\n",
        "        archivo_csv_salida (str): Nombre del archivo CSV de salida (ej: 'llenado_O3.CSV').\n",
        "    \"\"\"\n",
        "    print(f'\\nProcesando {archivo_csv_entrada}...')\n",
        "\n",
        "    try:\n",
        "        # 1. Leer el CSV, especificando la primera columna como índice,\n",
        "        #    PERO SIN intentar parsear fechas automáticamente aquí.\n",
        "        #    El índice será inicialmente de tipo 'Index' conteniendo strings.\n",
        "        data = pd.read_csv(archivo_csv_entrada, index_col=0)\n",
        "        print(\"Archivo leído inicialmente.\")\n",
        "        print(f\"Tipo de índice ANTES de conversión: {type(data.index)}\")\n",
        "        # Opcional: Muestra las primeras filas del índice para verificar visualmente\n",
        "        # print(\"Primeras 5 entradas del índice (como strings):\\n\", data.index[:5])\n",
        "\n",
        "        # 2. Definir el formato esperado de las fechas/horas en el archivo CSV\n",
        "        #    Basado en tu ejemplo '1/01/2013 0:00'\n",
        "        formato_fecha_esperado = '%d/%m/%Y %H:%M'\n",
        "        print(f\"Intentando convertir el índice a Datetime usando el formato: '{formato_fecha_esperado}'\")\n",
        "\n",
        "        # 3. Convertir explícitamente el índice (que contiene los strings de 'FECHA') a DatetimeIndex\n",
        "        #    errors='coerce': Si alguna cadena no coincide EXACTAMENTE con el formato,\n",
        "        #                     se convertirá en NaT (Not a Time) en lugar de detener el script.\n",
        "        original_index = data.index # Guardar una copia por si acaso\n",
        "        data.index = pd.to_datetime(data.index, format=formato_fecha_esperado, errors='coerce')\n",
        "\n",
        "        print(f\"Tipo de índice DESPUÉS de conversión: {type(data.index)}\")\n",
        "\n",
        "        # 4. Verificar si la conversión fue exitosa y si hubo errores (NaT)\n",
        "        if isinstance(data.index, pd.DatetimeIndex):\n",
        "            print(\"Conversión a DatetimeIndex realizada.\")\n",
        "            nans_en_indice = data.index.isna().sum()\n",
        "            if nans_en_indice > 0:\n",
        "                print(f\"\\n¡ADVERTENCIA IMPORTANTE!: Se encontraron {nans_en_indice} fechas/horas en la columna 'FECHA'\")\n",
        "                print(f\"que NO pudieron ser interpretadas usando el formato '{formato_fecha_esperado}'.\")\n",
        "                print(\"Estas entradas se han convertido a 'NaT' (Not a Time) en el índice.\")\n",
        "                print(\"Esto puede afectar los cálculos y la imputación.\")\n",
        "                print(\"-> REVISA tu archivo CSV original en las filas correspondientes a los NaT para corregir el formato.\")\n",
        "                # Podrías querer ver cuáles fallaron (esto puede ser largo si hay muchas):\n",
        "                # print(\"Índices originales que fallaron la conversión:\")\n",
        "                # print(original_index[data.index.isna()])\n",
        "                # Decide si continuar o detenerte. Por ahora, continuaremos pero los NaT no se imputarán bien.\n",
        "            else:\n",
        "                print(\"Todas las fechas/horas del índice se convirtieron correctamente.\")\n",
        "\n",
        "        else:\n",
        "            # Si la conversión falló completamente y el índice NO es DatetimeIndex\n",
        "            print(\"\\nError CRÍTICO: La conversión explícita a DatetimeIndex falló.\")\n",
        "            print(f\"El índice sigue siendo de tipo: {type(data.index)}\")\n",
        "            print(\"Asegúrate de que TODAS las entradas en la primera columna ('FECHA')\")\n",
        "            print(f\"sigan EXACTAMENTE el formato '{formato_fecha_esperado}' (ej: '1/01/2013 0:00').\")\n",
        "            print(\"Incluso un solo valor con formato diferente puede causar este fallo si 'errors' no es 'coerce'.\")\n",
        "            return # Detener la ejecución\n",
        "\n",
        "        # --- Si llegamos aquí, data.index ES un DatetimeIndex (aunque pueda tener NaTs) ---\n",
        "\n",
        "        print(f\"\\nColumnas encontradas: {data.columns.tolist()}\")\n",
        "        print(f\"Dimensiones originales: {data.shape}\")\n",
        "        # Recalcular NaNs por si la conversión a NaT afectó filas enteras (poco probable aquí)\n",
        "        print(f\"Valores faltantes ANTES de imputar (por columna):\\n{data.isnull().sum()}\")\n",
        "\n",
        "        # Calcular promedios horarios (esto ignorará filas con índice NaT si las hubiera)\n",
        "        # Usamos .dt.hour que funciona sobre DatetimeIndex\n",
        "        promedios_horas = data.groupby(data.index.dt.hour).mean()\n",
        "        print(\"\\nPromedios horarios calculados.\")\n",
        "        # print(promedios_horas)\n",
        "\n",
        "        data_imputada = data.copy()\n",
        "\n",
        "        print(\"Iniciando imputación por hora (puede tardar)...\")\n",
        "        suma = 0\n",
        "        num_columnas = len(data.columns)\n",
        "        estaciones_procesadas = 0\n",
        "        imputaciones_realizadas = 0\n",
        "\n",
        "        for name_estacion in data.columns:\n",
        "            estaciones_procesadas += 1\n",
        "            progress = int(estaciones_procesadas / num_columnas * 100)\n",
        "            sys.stdout.write('\\r')\n",
        "            sys.stdout.write('[%-20s] %d%% Procesando: %s' % ('=' * int(progress/5), progress, name_estacion))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            for indice_tiempo in data_imputada.index:\n",
        "                 # Saltar si el índice mismo es NaT (fecha inválida)\n",
        "                 if pd.isna(indice_tiempo):\n",
        "                     continue\n",
        "\n",
        "                 # Verificar si el valor de datos es NaN\n",
        "                 if pd.isna(data_imputada.loc[indice_tiempo, name_estacion]):\n",
        "                    hora_int = indice_tiempo.hour # Acceder a la hora (entero)\n",
        "\n",
        "                    if hora_int in promedios_horas.index:\n",
        "                       valor_imputado = promedios_horas.loc[hora_int, name_estacion]\n",
        "                       if not pd.isna(valor_imputado):\n",
        "                           data_imputada.loc[indice_tiempo, name_estacion] = valor_imputado\n",
        "                           imputaciones_realizadas += 1\n",
        "\n",
        "        sys.stdout.write('\\n')\n",
        "        print(f\"Imputación por hora completada. Se realizaron {imputaciones_realizadas} imputaciones.\")\n",
        "\n",
        "        nans_restantes = data_imputada.isnull().sum()\n",
        "        total_nans_restantes = nans_restantes.sum()\n",
        "        if total_nans_restantes > 0:\n",
        "             print(f\"\\nAdvertencia: Quedaron {total_nans_restantes} valores NaN después de la imputación.\")\n",
        "             # Explicar posible causa de NaNs restantes\n",
        "             print(\"Causas posibles: (1) Todos los valores originales para una combinación estación/hora eran NaN.\")\n",
        "             print(\"                  (2) Filas donde la fecha original era inválida (índice NaT) no fueron imputadas.\")\n",
        "             print(f\"NaNs restantes por columna:\\n{nans_restantes[nans_restantes > 0]}\")\n",
        "        else:\n",
        "             print(\"\\nNo quedaron valores NaN después de la imputación horaria.\")\n",
        "\n",
        "        # Guardar el resultado\n",
        "        # Nota: Las filas con índice NaT (si las hubo) se guardarán con una entrada vacía\n",
        "        # o 'NaT' en la columna de índice del archivo CSV de salida.\n",
        "        data_imputada.to_csv(archivo_csv_salida)\n",
        "        print(f'\\nProceso completado. Archivo imputado guardado como: {archivo_csv_salida}')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error Fatal: El archivo '{archivo_csv_entrada}' no se encontró.\")\n",
        "        print(\"Por favor, asegúrate de que 'O3.CSV' esté en el mismo directorio que este script.\")\n",
        "    except ValueError as e:\n",
        "        # Captura errores si pd.to_datetime falla y no usamos errors='coerce'\n",
        "        print(f\"\\nError de Valor durante la conversión de fecha/hora: {e}\")\n",
        "        print(\"Esto generalmente significa que una o más cadenas en la columna de índice\")\n",
        "        print(f\"no coinciden con el formato esperado: '{formato_fecha_esperado}'.\")\n",
        "    except KeyError as e:\n",
        "         print(f\"\\nError de Clave: {e}.\")\n",
        "         print(\"Puede que una columna esperada no exista o falte la columna índice.\")\n",
        "         print(\"Asegúrate de que la primera columna es 'FECHA'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nOcurrió un error inesperado durante el procesamiento: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- Bloque de Ejecución Principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    nombre_archivo_entrada = 'O3.csv'\n",
        "    nombre_archivo_salida = 'llenado_O3.CSV'\n",
        "    imputar_modificado(nombre_archivo_entrada, nombre_archivo_salida)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4y-PSVLz04f",
        "outputId": "767aa47b-4db1-4098-db18-02090a3e7e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando O3.csv...\n",
            "Archivo leído inicialmente.\n",
            "Tipo de índice ANTES de conversión: <class 'pandas.core.indexes.base.Index'>\n",
            "Intentando convertir el índice a Datetime usando el formato: '%d/%m/%Y %H:%M'\n",
            "Tipo de índice DESPUÉS de conversión: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
            "Conversión a DatetimeIndex realizada.\n",
            "Todas las fechas/horas del índice se convirtieron correctamente.\n",
            "\n",
            "Columnas encontradas: ['BOLIVIA', 'BOSA', 'CARVAJAL', 'CENTRO DE ALTO RENDIMIENTO', 'CIUDAD BOLIVAR', 'FONTIBON', 'GUAYMARAL', 'JAZMIN', 'KENNEDY', 'LAS FERIAS', 'MIN AMBIENTE', 'PUENTE ARANDA', 'SAN CRISTOBAL', 'SUBA', 'TUNAL', 'USAQUEN', 'USME', 'COLINA', 'MOVIL FONTIBON']\n",
            "Dimensiones originales: (87648, 19)\n",
            "Valores faltantes ANTES de imputar (por columna):\n",
            "BOLIVIA                       76368\n",
            "BOSA                          85879\n",
            "CARVAJAL                      20767\n",
            "CENTRO DE ALTO RENDIMIENTO    23194\n",
            "CIUDAD BOLIVAR                70029\n",
            "FONTIBON                      56290\n",
            "GUAYMARAL                      5992\n",
            "JAZMIN                        70241\n",
            "KENNEDY                       28075\n",
            "LAS FERIAS                    13651\n",
            "MIN AMBIENTE                  23360\n",
            "PUENTE ARANDA                  8101\n",
            "SAN CRISTOBAL                 11201\n",
            "SUBA                          11546\n",
            "TUNAL                         11953\n",
            "USAQUEN                        6608\n",
            "USME                          73713\n",
            "COLINA                        74533\n",
            "MOVIL FONTIBON                77299\n",
            "dtype: int64\n",
            "\n",
            "Ocurrió un error inesperado durante el procesamiento: 'DatetimeIndex' object has no attribute 'dt'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-17-3f5b21b843f6>\", line 77, in imputar_modificado\n",
            "    promedios_horas = data.groupby(data.index.dt.hour).mean()\n",
            "                                   ^^^^^^^^^^^^^\n",
            "AttributeError: 'DatetimeIndex' object has no attribute 'dt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHRRh73REsYD"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def imputar(archivo_csv):\n",
        "    print('\\nProcesando', archivo_csv)\n",
        "\n",
        "    # Leer el archivo sin prefijo de carpeta\n",
        "    data = pd.read_csv(archivo_csv, index_col=0, parse_dates=True)\n",
        "    data_nan = data.fillna(999)\n",
        "    data[\"FECHA\"] = pd.to_datetime(data[\"FECHA\"], format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
        "\n",
        "    # Calcular promedios horarios\n",
        "    promedios_horas = data.groupby(data.index.strftime('%H')).mean()\n",
        "\n",
        "    suma = 0\n",
        "    for name_estacion in data.columns:\n",
        "        suma += 1\n",
        "        progress = int(suma / len(data.columns) * 100)\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('[%-20s] %d%%' % ('=' * (progress // 5), progress))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        for i in range(len(data_nan[name_estacion])):\n",
        "            if data_nan[name_estacion].iloc[i] == 999:\n",
        "                hora = int(data_nan.index[i].hour)\n",
        "                data_nan.at[data_nan.index[i], name_estacion] = promedios_horas[name_estacion][hora]\n",
        "\n",
        "    # Guardar el resultado sin prefijo de carpeta\n",
        "    output_file = 'llenado_' + archivo_csv\n",
        "    data_nan.to_csv(output_file)\n",
        "    print(f\"\\nArchivo guardado correctamente: {output_file}\")\n",
        "\n",
        "def mean_axis1(archivo_csv):\n",
        "    data = pd.read_csv(archivo_csv, index_col=0, parse_dates=True)\n",
        "    mean = data.mean(axis=1)\n",
        "    mean[np.isnan(mean)] = np.nanmean(mean)\n",
        "\n",
        "    mean_df = pd.DataFrame({'MEAN': mean})\n",
        "    output_file = 'mean_' + archivo_csv\n",
        "    mean_df.to_csv(output_file)\n",
        "    print(f\"\\nArchivo guardado correctamente: {output_file}\")\n",
        "\n",
        "def change_dates(filee):\n",
        "    df0 = pd.read_csv('O3.csv')\n",
        "    df1 = pd.read_csv(filee)\n",
        "    df1['FECHA'] = df0['FECHA']\n",
        "\n",
        "    df1.to_csv(filee, index=False)\n",
        "    print(f\"\\nArchivo modificado correctamente: {filee}\")\n"
      ],
      "metadata": {
        "id": "kwBCOlFXJ66r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "w6qGqlryEsYI",
        "outputId": "3cb0d46f-88c6-43b0-98c4-5d2c7157262d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando O3.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'FECHA'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'FECHA'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4ad79806867d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimputar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-0bf97d0c3fae>\u001b[0m in \u001b[0;36mimputar\u001b[0;34m(archivo_csv)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FECHA\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FECHA\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%d/%m/%Y %H:%M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coerce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Calcular promedios horarios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'FECHA'"
          ]
        }
      ],
      "source": [
        "imputar('O3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ysp8DR4EsYG"
      },
      "outputs": [],
      "source": [
        "def imputar(archivo_csv):\n",
        "    print(''); print('Procesando', archivo_csv)\n",
        "    data            = pd.read_csv('DATA_ORIGINAL/' + archivo_csv, index_col = 0, parse_dates=True)\n",
        "    data_nan        = data.fillna(999)\n",
        "    promedios_horas = data.groupby(data.index.strftime('%H')).mean()\n",
        "\n",
        "    suma = 0\n",
        "    for name_estacion in data.columns:\n",
        "        suma += 1; progress = int(suma/len(data.columns) * 100)\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('[%-20s] %d%%' % ('=' * progress, progress))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        for i in np.arange(0,len(data_nan[name_estacion]),1):\n",
        "            dato = data_nan[name_estacion][i]\n",
        "\n",
        "            if dato == 999:\n",
        "                hora = int(data_nan[name_estacion].index[i].hour)\n",
        "                data_nan[name_estacion][i] = promedios_horas[name_estacion][hora]\n",
        "    # QUITAR esta linea para tener todas las estaciones\n",
        "    #data_nan = pd.DataFrame(data_nan.mean(axis=1)).rename(columns={0: 'MEAN'})\n",
        "    data_nan.to_csv('SALIDAS/llenado_' + archivo_csv)\n",
        "\n",
        "def mean_axis1(archivo_csv):\n",
        "    data = pd.read_csv('DATA_ORIGINAL/' + archivo_csv, index_col = 0, parse_dates=True)\n",
        "    mean = data.mean(axis=1)\n",
        "    mean = np.array(mean)\n",
        "    inds = np.where(np.isnan(mean))\n",
        "    mean[inds] = np.nanmean(mean)\n",
        "    mean = pd.DataFrame({'MEAN':mean})\n",
        "    mean.to_csv('SALIDAS/llenado_' + archivo_csv)\n",
        "\n",
        "\n",
        "def change_dates(filee):\n",
        "    df0 = pd.read_csv('DATA_ORIGINAL/NO.csv')\n",
        "    df1 = pd.read_csv('DATA_ORIGINAL/'+filee)\n",
        "    df1['FECHA'] = df0['FECHA']\n",
        "    df1.to_csv('DATA_ORIGINAL/'+filee, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNqeKVSoEsYH"
      },
      "outputs": [],
      "source": [
        "Path('SALIDAS').mkdir(parents=True, exist_ok=True)\n",
        "lista_files = os.listdir('DATA_ORIGINAL/')\n",
        "\n",
        "chem = 'no'\n",
        "if chem == 'yes':\n",
        "    lista_files = ['VEL.csv']\n",
        "    for i in lista_files:\n",
        "        change_dates(i)\n",
        "        imputar(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjkUKSFeEsYJ",
        "outputId": "dc551725-31ca-48ef-bdd5-08a2e9acda99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-fd436de1f853>:47: RuntimeWarning: Mean of empty slice\n",
            "  u_wind = np.nanmean(ut,axis=1)\n",
            "<ipython-input-2-fd436de1f853>:50: RuntimeWarning: Mean of empty slice\n",
            "  v_wind = np.nanmean(vt,axis=1)\n"
          ]
        }
      ],
      "source": [
        "data_ws = pd.read_csv('SALIDAS/llenado_VEL.csv')\n",
        "change_dates('DIR.csv')\n",
        "data_dir = pd.read_csv('DATA_ORIGINAL/DIR.csv',index_col=0,parse_dates=True)\n",
        "\n",
        "wind = components(data_ws,data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLZxB428EsYK"
      },
      "outputs": [],
      "source": [
        "def components(ws,wd):\n",
        "\n",
        "    ut = np.ones((len(wd),len(wd.columns)))\n",
        "    vt = np.ones((len(wd),len(wd.columns)))\n",
        "\n",
        "    for i,col in enumerate(wd.columns):\n",
        "        #Convert to components!\n",
        "        u_wind = np.array(ws['MEAN']) * np.sin(np.array(wd[col]) * np.pi/180)\n",
        "        v_wind = np.array(ws['MEAN']) * np.cos(np.array(wd[col]) * np.pi/180)\n",
        "        #Add the components to a matrix\n",
        "        ut[:,i] = u_wind\n",
        "        vt[:,i] = v_wind\n",
        "\n",
        "    #Calculate the stations mean and replace the nan values!\n",
        "    u_wind = np.nanmean(ut,axis=1)\n",
        "    inds = np.where(np.isnan(u_wind))\n",
        "    u_wind[inds] = np.nanmean(u_wind)\n",
        "    v_wind = np.nanmean(vt,axis=1)\n",
        "    inds = np.where(np.isnan(v_wind))\n",
        "    v_wind[inds] = np.nanmean(v_wind)\n",
        "\n",
        "    ## Add U\n",
        "    df0 = pd.DataFrame(np.column_stack((ws['FECHA'],u_wind)), columns=['FECHA','MEAN'])\n",
        "    df0.to_csv('SALIDAS/llenado_u-component.csv',index=False)\n",
        "\n",
        "    ## Add V\n",
        "    df1 = pd.DataFrame(np.column_stack((ws['FECHA'],v_wind)), columns=['FECHA','MEAN'])\n",
        "    df1.to_csv('SALIDAS/llenado_v-component.csv',index=False)\n",
        "\n",
        "    #Calculate the wind direction using the wind components!\n",
        "    mean_wd = np.arctan2(u_wind, v_wind) * 180/np.pi\n",
        "    #Result is from -180 to 180 due to arctan2, this move it between 0 to 360\n",
        "    mean_wd = (180 + mean_wd)\n",
        "    df2 = pd.DataFrame(np.column_stack((ws['FECHA'],mean_wd)), columns=['FECHA','MEAN'])\n",
        "    df2.to_csv('SALIDAS/llenado_DIR.csv',index=False)\n",
        "    return(mean_wd)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}